{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Missing Values\n",
        "\n",
        "'Missingness' Terminology:\n",
        "*   **MCAR:** Missing completely at random\n",
        "*   **MAR:** Missing at random - corruption is conditioned on other column\n",
        "*   **MNAR:** Missing not at random - corruption is conditioned on values in column on which it is applied"
      ],
      "metadata": {
        "id": "MWPkFUHGuGUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jenga.corruptions.generic import MissingValues\n",
        "\n",
        "def missing_values(df, column, fraction=.5, missingness='MCAR'):\n",
        "    df[column] = MissingValues(column=column, fraction=fraction, missingness=missingness).transform(df)[column]\n",
        "    return df"
      ],
      "metadata": {
        "id": "1-e3jxwmkoRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jenga.basis import DataCorruption\n",
        "class MillisInsteadOfSeconds ( DataCorruption ):\n",
        "\n",
        "    def transform ( self , data, column ):\n",
        "        self.fraction = 0.2\n",
        "        self.column = column\n",
        "        # Operate on a copy of the data\n",
        "        corrupted_data = data.copy( deep = True )\n",
        "        # Pick a random fraction of the rows\n",
        "        # Multiply the column values of the chosen rows\n",
        "        corrupted_data[self.column] = corrupted_data[self.column] * 1000\n",
        "        return corrupted_data\n",
        "\n",
        "def second_to_millis(df, column):\n",
        "  tranfomer = MillisInsteadOfSeconds()\n",
        "  df = df.ffill()\n",
        "  df = tranfomer.transform(df, column)\n",
        "  return df"
      ],
      "metadata": {
        "id": "RALj9XiWkkBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Gaussian Noise and rounding"
      ],
      "metadata": {
        "id": "WSx2umfOzpmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jenga.corruptions.numerical import GaussianNoise\n",
        "\n",
        "def noise_and_rounding(df, column, fraction=.5, missingness='MCAR'):\n",
        "  df[column] = df[column].astype(str)\n",
        "  parts = df[column].str.extract(r'^(\\D*?)(\\d+\\.*\\d*)(.*)')\n",
        "  parts[1] = parts[1].astype(float)\n",
        "  df[column] = GaussianNoise(column=column, fraction=fraction, sampling=missingness).transform(df)[column]\n",
        "  df[column] = df[column].round(0).astype(int)\n",
        "  combined_values = parts[0] + parts[1].astype(str) + parts[2]\n",
        "  df[column] = combined_values\n",
        "  return df"
      ],
      "metadata": {
        "id": "Ml8vBPNmzgRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Conversion"
      ],
      "metadata": {
        "id": "Irm8CfPimJph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jenga.basis import TabularCorruption\n",
        "\n",
        "# transforms string data for height in feet and inches into corresponding height in meters and cm\n",
        "class UnitTransform(TabularCorruption):\n",
        "    #Imperial to metric\n",
        "    def convert_to_metric(self, height):\n",
        "      if 'ft' in height and 'in' in height:\n",
        "        feet, inches = height.split(' ')\n",
        "        feet_cm = float(feet.replace(\"'\",\"\")) * 30.48\n",
        "        inches_cm = float(inches.replace('\"',\"\")) * 2.54\n",
        "        height_in_cm = round(feet_cm + inches_cm)\n",
        "        cm = height_in_cm%100\n",
        "        meter = round((height_in_cm - cm)/100)\n",
        "\n",
        "        return str(meter) + \"m \" + str(cm) + \"cm\"\n",
        "      else:\n",
        "        return height\n",
        "\n",
        "    def transform(self, data):\n",
        "      df = data.copy(deep=True)\n",
        "\n",
        "      if self.fraction > 0:\n",
        "          rows = self.sample_rows(data)\n",
        "          df.loc[rows, self.column] = df.loc[rows, self.column].apply(lambda x: self.convert_to_metric(x))\n",
        "\n",
        "      return df"
      ],
      "metadata": {
        "id": "wf7RdCxsUe3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def Unit_inch_to_cm(df, column, fraction=.5, missingness='MCAR'):\n",
        "  df[column] = UnitTransform(column=column, fraction=fraction, sampling=missingness).transform(df)[column]\n",
        "  return df"
      ],
      "metadata": {
        "id": "Dr-RbpWMYda-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Errors"
      ],
      "metadata": {
        "id": "NtAi3RZF3io1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Textattack\n",
        "This is system specific and unstable"
      ],
      "metadata": {
        "id": "JUerVTynRjMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "c1QJ39e02WFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textattack"
      ],
      "metadata": {
        "id": "gWKGTjXF4RWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Augmenter\n",
        "Some of the code below is copied from the textattack docs"
      ],
      "metadata": {
        "id": "N7hfIT42R2_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.augmentation import EmbeddingAugmenter\n",
        "augmenter = EmbeddingAugmenter()\n",
        "s = 'Hello my friend, what is your mission?'\n",
        "augmenter.augment(s)"
      ],
      "metadata": {
        "id": "96mYFib83vaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.augmentation import CheckListAugmenter\n",
        "cl = CheckListAugmenter(pct_words_to_swap=0.2, transformations_per_example=5)\n",
        "cl.augment(s)"
      ],
      "metadata": {
        "id": "mgxyPdc7Aele"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old Typo Injection"
      ],
      "metadata": {
        "id": "WoOt3qDDGF6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
        "from sklearn import preprocessing\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "kyvVrVhOQ6h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.attack_recipes import pruthi_2019"
      ],
      "metadata": {
        "id": "7ei46eRMGLe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textattack.models.wrappers import SklearnModelWrapper\n",
        "model_wrapper = SklearnModelWrapper(model,model_fit)"
      ],
      "metadata": {
        "id": "8MuneFSKMW_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "goal_function = textattack.goal_functions.UntargetedClassification(model_wrapper)\n",
        "attack = pruthi_2019.Pruthi2019.build(model_wrapper)"
      ],
      "metadata": {
        "id": "os6ED0t5Nqx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_cvect(training, testing, column_name):\n",
        "    vect = CountVectorizer(\n",
        "        max_features=100, ngram_range=(1, 3), stop_words=list(ENGLISH_STOP_WORDS)\n",
        "    )\n",
        "    vectFit = vect.fit(training[column_name])\n",
        "    cvect_training = vectFit.transform(training[column_name])\n",
        "    cvect_training_df = pd.DataFrame(\n",
        "        cvect_training.toarray(), columns=vect.get_feature_names_out()\n",
        "    )\n",
        "    cvect_testing = vectFit.transform(testing[column_name])\n",
        "    cvect_testing_df = pd.DataFrame(\n",
        "        cvect_testing.toarray(), columns=vect.get_feature_names_out()\n",
        "    )\n",
        "    return vectFit, cvect_training_df, cvect_testing_df"
      ],
      "metadata": {
        "id": "HwhmjVBzRDAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_tfidf(training, testing, column_name):\n",
        "    Tfidf = TfidfVectorizer(\n",
        "        ngram_range=(1, 3), max_features=100, stop_words=list(ENGLISH_STOP_WORDS)\n",
        "    )\n",
        "    Tfidf_fit = Tfidf.fit(training[column_name])\n",
        "    Tfidf_training = Tfidf_fit.transform(training[column_name])\n",
        "    Tfidf_training_df = pd.DataFrame(\n",
        "        Tfidf_training.toarray(), columns=Tfidf.get_feature_names()\n",
        "    )\n",
        "    Tfidf_testing = Tfidf_fit.transform(testing[column_name])\n",
        "    Tfidf_testing_df = pd.DataFrame(\n",
        "        Tfidf_testing.toarray(), columns=Tfidf.get_feature_names()\n",
        "    )\n",
        "    return Tfidf_fit, Tfidf_training_df, Tfidf_testing_df"
      ],
      "metadata": {
        "id": "uiHNQe_hQabV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_example_model(X_train, y_train, X_test, y_test, name_of_test):\n",
        "    rf = RandomForestClassifier(max_depth=4).fit(X_train, y_train)\n",
        "    y_pred = rf.predict(X_test)\n",
        "    print(\n",
        "        \"Training accuracy of \" + name_of_test + \": \", rf.score(X_train, y_train)\n",
        "    )\n",
        "    print(\"Testing accuracy of \" + name_of_test + \": \", rf.score(X_test, y_test))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    return rf"
      ],
      "metadata": {
        "id": "tK5VnkhTWgiu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_train, text_test = train_test_split(df)\n",
        "cvectFit, cvect_training_df, cvect_testing_df = transform_cvect(text_train,text_test,'review_text')\n",
        "example_model = build_example_model(text_train,text_train['fit'],text_test,text_test['fit'],'test')"
      ],
      "metadata": {
        "id": "KIC_tNbZPwQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old Typo Fixing"
      ],
      "metadata": {
        "id": "uhkMLlW3GJhb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SLOW experimental model from https://huggingface.co/oliverguhr/spelling-correction-english-base\n",
        "from transformers import pipeline\n",
        "fix_spelling = pipeline(\"text2text-generation\",model=\"oliverguhr/spelling-correction-english-base\")\n",
        "print(fix_spelling(\"lets do a comparsion\",max_length=2048))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X9s-CBxcFf3",
        "outputId": "28c3bc69-72f8-4a09-a708-e92e2e29dea1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \"Let's do a comparison.\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import neuspell\n",
        "from neuspell.neuspell import available_checkers\n",
        "print(f\"available checkers: {available_checkers()}\")"
      ],
      "metadata": {
        "id": "_qB84IzpftSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neuspell.neuspell import BertChecker\n",
        "checker = BertChecker()\n",
        "checker.from_pretrained()\n",
        "checker.correct(\"I luk foward to receving your reply\")"
      ],
      "metadata": {
        "id": "Mxi17k8ZOzMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from neuspell.neuspell import CnnlstmChecker\n",
        "checker = NestedlstmChecker()\n",
        "checker.from_pretrained()\n",
        "checker.correct(\"I luk foward to receving your reply\")"
      ],
      "metadata": {
        "id": "XHonBqfjgZ9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from autocorrect import Speller\n",
        "\n",
        "def fix_typo_autocorrect(df, columns):\n",
        "  spell = Speller(lang='en')\n",
        "  for column in columns:\n",
        "    df[column] = df[column].apply(spell)\n",
        "  return df"
      ],
      "metadata": {
        "id": "faDWxV9pvZRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summaries = df_typo['review_summary'].to_list()\n",
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')\n",
        "start = time.time()\n",
        "for i in range(len(summaries)):\n",
        "  if (i%50 == 0):\n",
        "    t = (time.time()) - start\n",
        "    print(str(i)+\"/\"+str(len(summaries))+ \"in \" +str(t))\n",
        "  summaries[i] = spell(summaries[i])"
      ],
      "metadata": {
        "id": "1UFOqENYwX_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "path = \"murali1996/bert-base-cased-spell-correction\"\n",
        "config = AutoConfig.from_pretrained(path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path)\n",
        "bert_model = AutoModelForTokenClassification.from_pretrained(path, config=config)\n",
        "model_dict = bert_model.state_dict()"
      ],
      "metadata": {
        "id": "oLRIaqyGOlY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_model.eval()"
      ],
      "metadata": {
        "id": "FEUEh3gCOppa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_vocab_dict(path_: str):\n",
        "    \"\"\"\n",
        "    path_: path where the vocab pickle file is saved\n",
        "    \"\"\"\n",
        "    with open(path_, 'rb') as fp:\n",
        "        vocab = pickle.load(fp)\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def _tokenize_untokenize(input_text: str, bert_tokenizer):\n",
        "    subtokens = bert_tokenizer.tokenize(input_text)\n",
        "    output = []\n",
        "    for subt in subtokens:\n",
        "        if subt.startswith(\"##\"):\n",
        "            output[-1] += subt[2:]\n",
        "        else:\n",
        "            output.append(subt)\n",
        "    return \" \".join(output)\n",
        "\n",
        "\n",
        "def _custom_bert_tokenize_sentence(input_text, bert_tokenizer, max_len):\n",
        "    tokens = []\n",
        "    split_sizes = []\n",
        "    text = []\n",
        "    for token in _tokenize_untokenize(input_text, bert_tokenizer).split(\" \"):\n",
        "        word_tokens = bert_tokenizer.tokenize(token)\n",
        "        if len(tokens) + len(word_tokens) > max_len - 2:  # 512-2 = 510\n",
        "            break\n",
        "        if len(word_tokens) == 0:\n",
        "            continue\n",
        "        tokens.extend(word_tokens)\n",
        "        split_sizes.append(len(word_tokens))\n",
        "        text.append(token)\n",
        "\n",
        "    return \" \".join(text), tokens, split_sizes\n",
        "\n",
        "\n",
        "def _custom_bert_tokenize(batch_sentences, bert_tokenizer, padding_idx=None, max_len=512):\n",
        "    if padding_idx is None:\n",
        "        padding_idx = bert_tokenizer.pad_token_id\n",
        "\n",
        "    out = [_custom_bert_tokenize_sentence(text, bert_tokenizer, max_len) for text in batch_sentences]\n",
        "    batch_sentences, batch_tokens, batch_splits = list(zip(*out))\n",
        "    batch_encoded_dicts = [bert_tokenizer.encode_plus(tokens) for tokens in batch_tokens]\n",
        "    batch_input_ids = pad_sequence(\n",
        "        [torch.tensor(encoded_dict[\"input_ids\"]) for encoded_dict in batch_encoded_dicts], batch_first=True,\n",
        "        padding_value=padding_idx)\n",
        "    batch_attention_masks = pad_sequence(\n",
        "        [torch.tensor(encoded_dict[\"attention_mask\"]) for encoded_dict in batch_encoded_dicts], batch_first=True,\n",
        "        padding_value=0)\n",
        "    batch_bert_dict = {\"attention_mask\": batch_attention_masks,\n",
        "                       \"input_ids\": batch_input_ids\n",
        "                       }\n",
        "    return batch_sentences, batch_bert_dict, batch_splits\n",
        "\n",
        "\n",
        "def _custom_get_merged_encodings(bert_seq_encodings, seq_splits, mode='avg', keep_terminals=False, device=\"cpu\"):\n",
        "    bert_seq_encodings = bert_seq_encodings[:sum(seq_splits) + 2, :]  # 2 for [CLS] and [SEP]\n",
        "    bert_cls_enc = bert_seq_encodings[0:1, :]\n",
        "    bert_sep_enc = bert_seq_encodings[-1:, :]\n",
        "    bert_seq_encodings = bert_seq_encodings[1:-1, :]\n",
        "    # a tuple of tensors\n",
        "    split_encoding = torch.split(bert_seq_encodings, seq_splits, dim=0)\n",
        "    batched_encodings = pad_sequence(split_encoding, batch_first=True, padding_value=0)\n",
        "    if mode == 'avg':\n",
        "        seq_splits = torch.tensor(seq_splits).reshape(-1, 1).to(device)\n",
        "        out = torch.div(torch.sum(batched_encodings, dim=1), seq_splits)\n",
        "    elif mode == \"add\":\n",
        "        out = torch.sum(batched_encodings, dim=1)\n",
        "    elif mode == \"first\":\n",
        "        out = batched_encodings[:, 0, :]\n",
        "    else:\n",
        "        raise Exception(\"Not Implemented\")\n",
        "\n",
        "    if keep_terminals:\n",
        "        out = torch.cat((bert_cls_enc, out, bert_sep_enc), dim=0)\n",
        "    return out"
      ],
      "metadata": {
        "id": "dm08SG2wOsZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    misspelled_sentences = [\"Well,becuz badd spelln is ard to undrstnd wen ou rid it.\",\n",
        "                            \"they fought a deadly waer\",\n",
        "                            \"Hurahh!! we mad it....\"]\n",
        "    batch_sentences, batch_bert_dict, batch_splits = _custom_bert_tokenize(misspelled_sentences, tokenizer)\n",
        "    # print(batch_sentences, \"\\n\")\n",
        "    outputs = bert_model(batch_bert_dict['input_ids'], attention_mask=batch_bert_dict[\"attention_mask\"],\n",
        "                          output_hidden_states=True)\n",
        "    sequence_output = outputs[1][-1]\n",
        "    # sanity check -------->\n",
        "    # sequence_output = bert_model.dropout(sequence_output)\n",
        "    # temp_logits = bert_model.classifier(sequence_output)\n",
        "    # x1 = [val.data for val in outputs[0].reshape(-1,)]\n",
        "    # x2 = [val.data for val in temp_logits.reshape(-1,)]\n",
        "    # assert all([a == b for a, b in zip(x1, x2)])\n",
        "    # <-------- sanity check\n",
        "    bert_encodings_splitted = \\\n",
        "        [_custom_get_merged_encodings(bert_seq_encodings, seq_splits, mode='avg')\n",
        "          for bert_seq_encodings, seq_splits in zip(sequence_output, batch_splits)]\n",
        "    bert_merged_encodings = pad_sequence(bert_encodings_splitted,\n",
        "                                          batch_first=True,\n",
        "                                          padding_value=0\n",
        "                                          )  # [BS,max_nwords_without_cls_sep,768]\n",
        "    logits = bert_model.classifier(bert_merged_encodings)\n",
        "    output_vocab = load_vocab_dict(\"vocab.pkl\")\n",
        "    # print(logits.shape)\n",
        "    assert len(output_vocab[\"idx2token\"]) == logits.shape[-1]\n",
        "    argmax_inds = torch.argmax(logits, dim=-1)\n",
        "    outputs = [\" \".join([output_vocab[\"idx2token\"][idx.item()] for idx in argmaxs][:len(wordsplits)])\n",
        "                for wordsplits, argmaxs in zip(batch_splits, argmax_inds)]\n",
        "    print(outputs)\n",
        "\n",
        "    print(\"complete\")"
      ],
      "metadata": {
        "id": "m5ElrOt6OuM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "start = time.time()\n",
        "df_typo.iloc['review_text']=df_typo..apply(spell)\n",
        "stop = time.time()\n",
        "print(f\"Typo fix time: {stop - start}s\")"
      ],
      "metadata": {
        "id": "LGsb2rsWO8NG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spellwise import (CaverphoneOne, CaverphoneTwo, Editex, Levenshtein,\n",
        "                       Soundex, Typox)\n",
        "algorithm = Editex()\n",
        "\n",
        "algorithm.add_words(words)\n",
        "suggestions = algorithm.get_suggestions(\"lovily\")\n",
        "suggestions[0]['word']"
      ],
      "metadata": {
        "id": "_SaWNv0-PEAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Old Typo Test"
      ],
      "metadata": {
        "id": "r0XX6XlvNTCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def small_typo_dfs(base_df, columns, target, typo_probs, corrector, max_target_sample = 10000, random_state=1, verbose = True):\n",
        "  df = base_df.filter(columns + [target])\n",
        "  target_values = df[target].unique()\n",
        "  target_sample_size = min(min(df[target].value_counts()),max_target_sample)\n",
        "  sampled_df = pd.DataFrame()\n",
        "  for tval in target_values:\n",
        "    sampled_df = pd.concat([sampled_df,df.loc[df[target]==tval].sample(n=target_sample_size,random_state=random_state)])\n",
        "  sampled_df = sampled_df.sample(frac=1,random_state=random_state).reset_index(drop=True)\n",
        "  typo_dfs = []\n",
        "  if verbose:\n",
        "    print(\"Injecting typos...\")\n",
        "  for prob in typo_probs:\n",
        "    if(prob == 0):\n",
        "      typo_dfs.append(sampled_df)\n",
        "    else:\n",
        "      new_df = sampled_df.copy()\n",
        "      new_df = naive_typo_df(new_df,columns,prob)\n",
        "      typo_dfs.append(new_df)\n",
        "  if verbose:\n",
        "    print(\"Fixing typos...\")\n",
        "  fixed_dfs = []\n",
        "  for i in range(len(typo_dfs)):\n",
        "    if(typo_probs[i] == 0):\n",
        "      fixed_dfs.append(sampled_df)\n",
        "    else:\n",
        "      new_df = typo_dfs[i].copy()\n",
        "      new_df = correct_typo(new_df,columns,corrector,verbose)\n",
        "      fixed_dfs.append(new_df)\n",
        "  if verbose:\n",
        "    print(\"Fixed typos. Done.\")\n",
        "  return typo_dfs, fixed_dfs"
      ],
      "metadata": {
        "id": "E1b-rhpeNPmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('renttherunway_final_data.json', lines=True)\n",
        "columns = ['review_text','review_summary']\n",
        "typo_dfs, fixed_dfs = small_typo_dfs(df,columns,'fit',[0,0.05,0.1],corrector,3000)"
      ],
      "metadata": {
        "id": "oTp78YfxNYWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_count(df, column):\n",
        "  return (df[column].str.count(' ')+1).sum()"
      ],
      "metadata": {
        "id": "vY5wg0aoNYu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical Data Fixes"
      ],
      "metadata": {
        "id": "7nJsRbTaNq1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reduce Category Data Duplication"
      ],
      "metadata": {
        "id": "ujnKSf3iNuCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from strsimpy import Jaccard,Cosine\n",
        "\n",
        "def reduce_category(df):\n",
        "    df2 = df.copy()\n",
        "    df2 = df2.drop(df2.columns.difference(['category']), axis=1)\n",
        "\n",
        "    # Handling quality\n",
        "    missing_rows = df2[df2['category'].isnull()].index\n",
        "    df2.drop(missing_rows, axis = 0, inplace=True)\n",
        "\n",
        "    df2 = pd.DataFrame({'unique_category': df2['category'].unique()})\n",
        "\n",
        "    # Define threshold for cosine and Jaccard similarity\n",
        "    cosine_threshold = 0.7\n",
        "    jaccard_threshold = 0.7\n",
        "\n",
        "    c=Cosine(1)\n",
        "    jaccard=Jaccard(1)\n",
        "\n",
        "    # Group similar tokens based on both cosine and Jaccard similarity\n",
        "    groups = {}\n",
        "    for i, category in enumerate(df2['unique_category']):\n",
        "        similar_indices = [j for j, other_category in enumerate(df2['unique_category']) if 1-jaccard.distance(category, other_category) > jaccard_threshold\n",
        "                                  and 1-c.distance(category, other_category) > cosine_threshold]\n",
        "        similar_indices = np.unique((similar_indices), axis=None)\n",
        "        groups[category] = [df2['unique_category'].iloc[j] for j in similar_indices]\n",
        "\n",
        "\n",
        "    # Map each original token to its corresponding group\n",
        "    token_to_group = {}\n",
        "    for category, similar_categories in groups.items():\n",
        "        for similar_category in similar_categories:\n",
        "            token_to_group[similar_category] = category\n",
        "\n",
        "    # Replace original tokens with group tokens\n",
        "    df2['reduced_category'] = [token_to_group[category] for category in df2['unique_category']]\n",
        "\n",
        "    mapping_dict = dict(zip(df2['unique_category'], df2['reduced_category']))\n",
        "    df['category'] = df['category'].map(mapping_dict)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "GRdd0Gx5N1MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Category Imputation"
      ],
      "metadata": {
        "id": "StFxvZL7Nz1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import miceforest as mf\n",
        "import pandas as pd\n",
        "\n",
        "def apply_categorical_imputation_I(df, categorical_columns1, categorical_columns2):\n",
        "    columns_to_drop = ['item_id', 'user_id', 'review_date', 'fit','review_text', 'review_summary','Year', 'Month', 'Day']\n",
        "    df2 = df.copy()\n",
        "    df2 = df2.drop(columns=columns_to_drop)\n",
        "\n",
        "    df2[categorical_columns1+categorical_columns2] = df2[categorical_columns1+categorical_columns2].astype('category')\n",
        "\n",
        "    # Create kernels.  #mice forest\n",
        "    kernel = mf.ImputationKernel(\n",
        "      data=df2,\n",
        "      save_all_iterations=True,\n",
        "      random_state=1343\n",
        "    )\n",
        "    # Run the MICE algorithm for 3 iterations on each of the datasets\n",
        "    kernel.mice(3,verbose=True, n_estimators=50)\n",
        "    #print(kernel)\n",
        "    # slowest speed, highest imputation quality for large datasets\n",
        "    completed_dataset = kernel.complete_data(dataset=0, inplace=False)\n",
        "\n",
        "\n",
        "    # Drop common columns from df\n",
        "    df.drop(columns=categorical_columns1+categorical_columns2, inplace=True)\n",
        "\n",
        "    # Replace dropped columns in df with columns from completed_dataset\n",
        "    df[categorical_columns1+categorical_columns2] = completed_dataset[categorical_columns1+categorical_columns2]\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "pfvb7eb-NsXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Autofix (Old)"
      ],
      "metadata": {
        "id": "K3qUQdwQRJSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from jenga.corruptions.generic import MissingValues\n",
        "from jenga.corruptions.generic import SwappedValues\n",
        "from jenga.corruptions.numerical import Scaling\n",
        "from jenga.corruptions.numerical import GaussianNoise\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "class metadata:\n",
        "    def determine_column_types(self,df):\n",
        "        self.categorical_features = {}\n",
        "        self.numerical_features = {}\n",
        "        self.text_features = {}\n",
        "        for x in df:\n",
        "            if(len(pd.unique(df[x])) != len(df[x])):\n",
        "                if(\"id\" in x):\n",
        "                    continue\n",
        "                if(is_numeric_dtype(df[x])):\n",
        "                    self.numerical_features[x] = {\"clean\": 100 - ((df[x].isna().sum() / len(df[x])) * 100), \"unique\" : len(pd.unique(df[x]))}\n",
        "                elif(len(pd.unique(df[x])) <= max(20,0.001 * len(df))):\n",
        "                    self.categorical_features[x] = {\"clean\": 100 - ((df[x].isna().sum() / len(df[x])) * 100), \"unique\" : len(pd.unique(df[x]))}\n",
        "                elif(isinstance(df[x].iloc[0], str)):\n",
        "                    self.text_features[x] = {\"clean\": 100 - ((df[x].isna().sum() / len(df[x])) * 100), \"unique\" : len(pd.unique(df[x]))}\n",
        "\n",
        "    def __init__(self,df):\n",
        "        self.determine_column_types(df)\n",
        "\n",
        "class autofix:\n",
        "    def preprocess(self):\n",
        "        for column in self.df:\n",
        "            if(isinstance(self.df[column].iloc[0], str)):\n",
        "                if(np.sum(self.df[column].str.isnumeric()) == len(self.df[column])):\n",
        "                    self.df[column] = self.df[column].astype(int)\n",
        "                elif(np.sum(self.df[column].replace(\".\", \"\").str.isnumeric()) == len(self.df[column])):\n",
        "                    self.df[column] = self.df[column].astype(float)\n",
        "\n",
        "    def bootstrap_imputer(self, df, column_to_predict, clean_mask):\n",
        "        features_to_subset = set(self.metadata.categorical_features.keys())\n",
        "        features_to_subset.discard(column_to_predict)\n",
        "        features_to_subset.discard(\"fit\")\n",
        "        features_to_subset.discard(\"height\")\n",
        "        dirty_mask = ~clean_mask\n",
        "        clean_data = df.loc[clean_mask]\n",
        "        clean_sizes = df.groupby(list(features_to_subset)).size().sort_values(ascending=False).reset_index()\n",
        "        NaN_data = df.loc[dirty_mask]\n",
        "        new_NaN_data = pd.DataFrame()\n",
        "        sizes_index = 0\n",
        "        while(clean_sizes.loc[sizes_index][0] > 5):\n",
        "            if(sizes_index > 100):\n",
        "                break\n",
        "            query = \"\"\n",
        "            for column in features_to_subset:\n",
        "                value = clean_sizes.loc[sizes_index][column]\n",
        "                is_numeric = isinstance(value,int) | isinstance(value, float)\n",
        "                if(is_numeric):\n",
        "                    query = query + f'{column} == {value} and '\n",
        "                else:\n",
        "                    query = query + f'{column} == \"{value}\" and '\n",
        "            query = query[:-4]\n",
        "            sample_space = clean_data.query(query)\n",
        "            impute_value = sample_space[column_to_predict].mode()[0]\n",
        "            data_to_impute = NaN_data.query(query)\n",
        "            NaN_data.loc[data_to_impute.index,column_to_predict] = impute_value\n",
        "            sizes_index += 1\n",
        "        other_data_index = NaN_data[column_to_predict].isna()\n",
        "        impute_value = clean_data[column_to_predict].mode()[0]\n",
        "        NaN_data.loc[other_data_index,column_to_predict] = impute_value\n",
        "        return pd.concat([clean_data, NaN_data])\n",
        "\n",
        "    def scaling_fix(self,scaling_factors = [1000,100], base_factor = 1):\n",
        "        for column in list(self.metadata.numerical_features.keys()):\n",
        "            min = np.min(self.df[column])\n",
        "            if(min == 0):\n",
        "                min  = 1\n",
        "            for factor in scaling_factors:\n",
        "                mask = self.df[column] >= factor * min\n",
        "                self.df.loc[mask, column] = self.df.loc[mask, column] / factor\n",
        "            self.df[column] = self.df[column] * base_factor\n",
        "\n",
        "    def impute_missing(self):\n",
        "        self.df = self.bootstrap_imputer(self.df, self.best_cat[0],self.df[self.best_cat[0]].notna())\n",
        "        for column in self.df:\n",
        "            if(column != self.best_cat[0]):\n",
        "                self.df= self.bootstrap_imputer(self.df, column, self.df[column].notna())\n",
        "\n",
        "    def __init__(self,df):\n",
        "        self.df = df.dropna(thresh = round(0.25 * len(df.columns)))\n",
        "        print(self.df)\n",
        "\n",
        "    def fix(self):\n",
        "        self.preprocess()\n",
        "        self.metadata = metadata(self.df)\n",
        "        self.non_text_columns = list(self.metadata.numerical_features.keys()) + list(self.metadata.categorical_features.keys())\n",
        "        self.best_cat = (\"\",0,0)\n",
        "        self.second_best_cat = (\"\",0,0)\n",
        "        for cat in list(self.metadata.categorical_features.keys()):\n",
        "            if ((self.metadata.categorical_features[cat][\"unique\"] > self.best_cat[1]) & (self.metadata.categorical_features[cat][\"clean\"] >= self.best_cat[2])):\n",
        "                self.second_best_cat = self.best_cat\n",
        "                self.best_cat = (cat, self.metadata.categorical_features[cat][\"unique\"],self.metadata.categorical_features[cat][\"clean\"])\n",
        "            elif ((self.metadata.categorical_features[cat][\"unique\"] > self.second_best_cat[1]) & (self.metadata.categorical_features[cat][\"clean\"] >= self.second_best_cat[2])):\n",
        "                self.second_best_cat = (cat, self.metadata.categorical_features[cat][\"unique\"],self.metadata.categorical_features[cat][\"clean\"])\n",
        "        self.impute_missing()\n",
        "        self.scaling_fix()\n",
        "        return self.df\n"
      ],
      "metadata": {
        "id": "Xp_rQb2nRLIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other"
      ],
      "metadata": {
        "id": "bRaChBSkltes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stats\n",
        "def print_stats(df):\n",
        "  number_of_rows = len(df.values)\n",
        "  print(\"Number of rows:\", number_of_rows)\n",
        "  missing_data = pd.DataFrame({'total_missing': df.isnull().sum(), 'perc_missing': (df.isnull().sum()/number_of_rows)*100})\n",
        "  print(missing_data)\n",
        "  return"
      ],
      "metadata": {
        "id": "jr0VYzEGN8_0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}